# Advanced-Learning-Algorithms
Practical and learning algorithm the decision tree with variations of the decision tree, including random forests and boosted trees (XGBoost).
# ğŸ‘‹ Hi, I'm Shashi Kant Oraon

## ğŸš€ Product Manager | Aspiring Machine Learning Practitioner

Iâ€™m a **Product Manager at Vivo Mobiles India**, building and scaling **D2C eâ€‘commerce platforms**. Alongside product strategy and execution, I actively invest in **Machine Learning fundamentals** to better design dataâ€‘driven products, personalization systems, pricing engines, and intelligent decision workflows.

---

## ğŸ“ Certifications

### âœ… Supervised Machine Learning: Regression and Classification

**Instructor:** Andrew Ng
**Platform:** Coursera
ğŸ“œ **Credential:** [View Certificate / Course]([https://www.coursera.org/learn/machine-learning](https://coursera.org/share/8079cac9f1eb9c0f8e097cce5de31961)

This certification strengthened my understanding of core supervised learning techniques and the mathematical intuition behind them, with handsâ€‘on implementation using Python.

---

## ğŸ“… Week-wise Learning Objectives (Andrew Ng â€“ Supervised Machine Learning & Neural Networks)

---

# ğŸ§  Neural Networks, Model Optimization, and Advanced ML Concepts

## ğŸ—“ï¸ Week 1: Neural Network Foundations

### ğŸ“š Detailed Learning Objectives

* Understand neural network architecture: input, hidden, and output layers
* Learn how neurons connect using weights and biases
* Understand feature learning across layers
* Calculate activations using mathematical functions
* Build neural networks using TensorFlow and pure Python
* Understand vectorized computation for performance optimization

### ğŸ§© Neural Network Component Diagram

```
Input Layer      Hidden Layer         Output Layer
xâ‚ â”€â”€â”€â”€â”€â”€â”€â–¶  â—‹ â”€â”€â”€â”€â”€â”€â”€â–¶ â—‹ â”€â”€â”€â”€â”€â”€â”€â–¶ Prediction
xâ‚‚ â”€â”€â”€â”€â”€â”€â”€â–¶  â—‹ â”€â”€â”€â”€â”€â”€â”€â–¶ â—‹
xâ‚ƒ â”€â”€â”€â”€â”€â”€â”€â–¶  â—‹ â”€â”€â”€â”€â”€â”€â”€â–¶ â—‹
```

### ğŸ§® Activation Calculation Formula

For neuron j:

zÊ² = Î£(wáµ¢xáµ¢) + b
aÊ² = g(zÊ²)

Vectorized form:

z = Wx + b
a = g(z)

---

### ğŸ–¼ï¸ Image Classification Feature Learning

```
Input Image â†’ Edge Detection â†’ Shape Detection â†’ Object Recognition

Layer 1 â†’ Learns edges
Layer 2 â†’ Learns shapes
Layer 3 â†’ Learns objects
```

---

### âš¡ Vectorization (Parallel Processing)

Without vectorization:

Loop through each example

With vectorization:

Z = XW + b

Faster and scalable computation

---

## ğŸ—“ï¸ Week 2: Training Neural Networks & Activation Functions

### ğŸ“š Detailed Learning Objectives

* Train neural networks using TensorFlow
* Understand activation function selection
* Implement multiclass classification
* Apply softmax and categorical cross entropy

### âš¡ Activation Functions Mathematical Forms

Sigmoid:

Ïƒ(z) = 1 / (1 + eâ»á¶»)

ReLU:

ReLU(z) = max(0, z)

Linear:

f(z) = z

---

### ğŸŒ Softmax Function

Used for multiclass classification:

softmax(záµ¢) = e^záµ¢ / Î£ e^zâ±¼

Example Output:

```
Class A â†’ 0.75
Class B â†’ 0.15
Class C â†’ 0.10
```

---

### ğŸ“‰ Categorical Cross Entropy Loss

J = âˆ’ Î£ yáµ¢ log(Å·áµ¢)

Measures prediction error.

---

## ğŸ—“ï¸ Week 3: Model Evaluation and Improvement

### ğŸ“š Detailed Learning Objectives

* Evaluate models using validation and test datasets
* Diagnose bias and variance issues
* Apply regularization
* Improve performance using transfer learning
* Use precision and recall metrics

---

### ğŸ“Š Dataset Splitting

```
Full Dataset
   â”‚
   â”œâ”€â”€ Training Set
   â”œâ”€â”€ Validation Set
   â””â”€â”€ Test Set
```

---

### ğŸ“‰ Regularization Formula

L2 Regularization:

J = Loss + (Î»/2m) Î£WÂ²

Prevents overfitting.

---

### ğŸ“Š Precision and Recall

Precision = TP / (TP + FP)

Recall = TP / (TP + FN)

```
High Precision â†’ Few False Positives
High Recall â†’ Few False Negatives
```

---

### ğŸ” ML Development Iteration Loop

```
Train â†’ Evaluate â†’ Diagnose â†’ Improve â†’ Retrain
```

---

## ğŸ—“ï¸ Week 4: Decision Trees and Ensemble Methods

### ğŸ“š Detailed Learning Objectives

* Understand decision tree structure and prediction logic
* Learn entropy and impurity calculation
* Understand ensemble methods: Random Forest and Boosting
* Compare neural networks vs decision trees

---

### ğŸŒ³ Decision Tree Prediction Flow

```
        Feature?
        /     \
     Yes       No
     /           \
  Predict A    Predict B
```

---

### ğŸ“‰ Entropy Formula

Entropy = âˆ’ Î£ p logâ‚‚(p)

Where:

* p = probability of class

Lower entropy = better classification split

---

### ğŸŒ² Random Forest Ensemble

```
Tree 1 â†’ Prediction A
Tree 2 â†’ Prediction B
Tree 3 â†’ Prediction A

Final Prediction â†’ Majority Vote
```

---

### âš–ï¸ When to Use Each Model

Neural Networks â†’ Images, speech, complex patterns
Decision Trees â†’ Structured data, interpretability

---

### ğŸ¯ Key Concepts

* Structure and components of neural networks
* Layers, neurons, weights, and biases
* Forward propagation and activation computation
* Image classification using neural networks
* Implementing neural networks using TensorFlow and Python

---

### ğŸ§© Neural Network Architecture

```
Input Layer        Hidden Layer         Output Layer
(xâ‚, xâ‚‚, xâ‚ƒ)  â†’   (aâ‚, aâ‚‚, aâ‚ƒ)   â†’     (Å·)

Each connection has:
Weight (w)
Bias (b)
```

Mathematical Representation:

z = wÂ·x + b
a = g(z)

Where:

* x = input features
* w = weights
* b = bias
* g(z) = activation function
* a = activation output

---

### ğŸ”„ Forward Propagation

```
zÂ¹ = WÂ¹x + bÂ¹
aÂ¹ = g(zÂ¹)

zÂ² = WÂ²aÂ¹ + bÂ²
Å· = g(zÂ²)
```

This process transforms inputs into predictions.

---

### ğŸ–¼ï¸ Neural Network for Image Classification

```
Image Pixels â†’ Hidden Layers â†’ Feature Learning â†’ Classification

Example:
Image â†’ Edge Detection â†’ Shape Detection â†’ Object Classification
```

---

## ğŸ—“ï¸ Week 2: Training Neural Networks & Activation Functions

### ğŸ¯ Key Concepts

* Training neural networks using TensorFlow
* Activation functions: Sigmoid, ReLU, Linear
* Multiclass classification using Softmax
* Cross-entropy loss

---

### âš¡ Activation Functions

#### Sigmoid

Ïƒ(z) = 1 / (1 + eâ»á¶»)

Used for:

* Binary classification output layer

```
     ______
   /
 /
/__________
```

---

#### ReLU (Rectified Linear Unit)

ReLU(z) = max(0, z)

Used for:

* Hidden layers

```
    /
   /
  /
_/________
```

---

### ğŸŒ Softmax for Multiclass Classification

Softmax formula:

P(y=i) = e^zi / Î£ e^zj

Example Output:

```
Cat   â†’ 0.7
Dog   â†’ 0.2
Bird  â†’ 0.1
```

---

### ğŸ“‰ Cross Entropy Loss

J = âˆ’ Î£ y log(Å·)

Used to measure classification error.

---

## ğŸ—“ï¸ Week 3: Improving Model Performance

### ğŸ¯ Key Concepts

* Cross validation and test sets
* Bias vs Variance diagnosis
* Regularization
* Error analysis
* Transfer learning
* Data augmentation
* Fairness and ethics

---

### ğŸ“Š Bias vs Variance

```
Underfit        Good Fit         Overfit
   /              __              _/\_
  /              /  \            /    \
```

High Bias â†’ Model too simple
High Variance â†’ Model too complex

---

### ğŸ“‰ Regularization

Regularized Cost Function:

J = original loss + (Î»/2m) Î£WÂ²

Purpose:

* Prevent overfitting
* Improve generalization

---

### ğŸ” Machine Learning Development Cycle

```
Train Model
    â†“
Evaluate
    â†“
Error Analysis
    â†“
Improve Data / Model
    â†“
Repeat
```

---

### ğŸ“Š Precision and Recall

Precision = TP / (TP + FP)

Recall = TP / (TP + FN)

Used for imbalanced datasets.

---

## ğŸ—“ï¸ Week 4: Decision Trees and Ensemble Methods

### ğŸ¯ Key Concepts

* Decision tree structure
* Entropy and impurity
* Random Forest and Boosted Trees
* When to use neural networks vs trees

---

### ğŸŒ³ Decision Tree Structure

```
        Age?
       /    \
     <30    â‰¥30
     /        \
  Buy       Income?
            /     \
         High     Low
         Buy     No Buy
```

---

### ğŸ“‰ Entropy Formula

Entropy measures impurity:

H(p) = âˆ’ Î£ p logâ‚‚(p)

Lower entropy = better split

---

### ğŸŒ² Random Forest

```
Tree 1 â†’ Prediction
Tree 2 â†’ Prediction
Tree 3 â†’ Prediction

Final Prediction â†’ Average / Majority Vote
```

---

## ğŸ§  Machine Learning Concepts Iâ€™ve Learned

### ğŸ”¹ Supervised Learning

Learning a function **f(x) â†’ y** from labeled data.

```
Input (X)  â”€â”€â–¶  Model  â”€â”€â–¶  Prediction (Å·)
                â”‚
                â–¼
             Loss
                â”‚
                â–¼
          Parameter Update
```

---

### ğŸ“ˆ Regression

Used to predict **continuous values** such as revenue, demand, or delivery time.

**Examples:**

* Sales forecasting
* Price elasticity modeling
* Demand prediction

**Linear Regression Flow:**

```
y = wâ‚xâ‚ + wâ‚‚xâ‚‚ + ... + b

Cost Function (MSE):
J(w,b) = (1/2m) Î£ (Å·áµ¢ âˆ’ yáµ¢)Â²
```

---

### ğŸ·ï¸ Classification

Used to predict **discrete labels**.

**Examples:**

* Fraud detection (Yes / No)
* Customer churn (Churn / Retain)
* Lead qualification

**Logistic Regression Pipeline:**

```
Linear Combination â†’ Sigmoid â†’ Probability â†’ Class

z = wÂ·x + b
Ïƒ(z) = 1 / (1 + eâ»á¶»)
```

---

### ğŸ“‰ Gradient Descent

An optimization algorithm to minimize the cost function.

```
Repeat until convergence:
  w = w âˆ’ Î± âˆ‚J/âˆ‚w
  b = b âˆ’ Î± âˆ‚J/âˆ‚b
```

```
Cost
 â–²
 |        â€¢
 |      â€¢
 |    â€¢
 |  â€¢
 |â€¢________________â–¶ Parameters
```

---

### ğŸ§ª Model Evaluation

Key metrics depending on problem type:

* **Regression:** MSE, RMSE, RÂ²
* **Classification:** Accuracy, Precision, Recall

```
Train Set â”€â–¶ Model â”€â–¶ Validate â”€â–¶ Improve
```

---

## ğŸ› ï¸ Tools & Skills

* **Languages:** Python
* **Libraries:** NumPy, Pandas, Matplotlib
* **ML Concepts:**

  * Linear & Logistic Regression
  * Cost Functions
  * Gradient Descent
  * Feature Scaling
  * Bias vs Variance

---

## ğŸ“¦ How I Apply ML Thinking as a Product Manager

* Designing **dataâ€‘backed experimentation frameworks**
* Translating business problems into **MLâ€‘solvable use cases**
* Collaborating effectively with **Data Science & Engineering teams**
* Evaluating model impact using **product metrics & ROI**

---

## ğŸŒ± What Iâ€™m Learning Next

* Advanced ML algorithms
* Feature engineering at scale
* Model deployment & monitoring
* ML useâ€‘cases in **Payments, Pricing & Personalization**

---

ğŸ“« **Letâ€™s connect and build intelligent products!**

> *â€œMachine learning is not about replacing decisionâ€‘makers, itâ€™s about empowering them.â€*
